This repository contains code to generate all figures and output for **"Taming the Curse of Dimensionality: Quantitative Economics with Deep Learning" (Jesús Fernández-Villaverde, Galo Nuño, Jesse Perla).**

It runs on all major operating systems and does not require any accelerators (e.g., GPUs). While you can use any Python environment manager, we recommend [uv](https://github.com/astral-sh/uv#installation), a faster and reproducible alternative to Conda, albeit with incomplete support for challenging binary dependencies.

# Replication Instructions

1. Install [uv](https://github.com/astral-sh/uv#installation), which is usually a one-line install, such as on macOS and Linux:
    ```bash
    curl -LsSf https://astral.sh/uv/install.sh | sh
    ```
     - On Windows: `winget install --id=astral-sh.uv  -e` or `powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"` 
     - If you have any installation issues, [see the docs](https://docs.astral.sh/uv/getting-started/installation/) for troubleshooting
2. Synchronize the environment. If you directly use `uv run python` as below, this step is done automatically.
    ```bash
    uv sync
    ```
3. Run the main script using one of two approaches:
    - **If you activated the environment:** See [here](https://docs.python.org/3/tutorial/venv.html#creating-virtual-environments) for activation details, then run:
        ```bash
        python generate_paper_figures_pytorch.py
        ```
    - **If you didn't activate:** Use `uv run` to automatically activate and then run:
        ```bash
        uv run python generate_paper_figures_pytorch.py
        ```

All output is in the `.figures` directory, including generated figures and a `results.json` that summarizes numerical values used in the paper.

**Note:** On the first run, most time is spent in the baseline solver using classic methods. Afterwards, results are cached and subsequent runs are much faster.

**Quick start on Linux or macOS:**
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
uv run python generate_paper_figures_pytorch.py
```

# Additional Instructions
The following section provides a few variations on the setup and execution.
## Activation in VS Code
Ensure you activate the Python environment (i.e., the `.venv` or conda virtual environment):
   - In VS Code, use `>Python: Select Interpreter` to select the local `.venv`
   - Outside VS Code, a platform-specific command will [activate .venv](https://docs.python.org/3/tutorial/venv.html#creating-virtual-environments) in your terminal.  


## Setup with Conda
If you prefer to use `conda` for your environment, then you can use the provided `requirements.txt`

```bash
conda create -n taming python=3.13
conda activate taming
pip install -r requirements.txt
```
## Self-contained Execution
You can run the code for a particular set of parameters directly on the commandline.  Again, it will cache the baseline results for any particular set of arguments so that subsequent runs are much faster.

The execution with the default arguments (as used in the paper and figures) is simply:
```bash
python stochastic_growth_pytorch.py
```

However, you can change parameters on the commandline. A few variations:
```bash
python stochastic_growth_pytorch.py --k_0_multiplier=0.9 --seed=53
python stochastic_growth_pytorch.py --mlp_width=128
python stochastic_growth_pytorch.py --opt_set.max_iter=15 --data_set.train_T=30
python stochastic_growth_pytorch.py --base_solver_set.num_z_points=41 --base_solver_set.num_k_points=100
```

Finally, to conduct experiments you can import the `stochastic_growth_pytorch` module and call `stochastic_growth` with whatever arguments you wish.  See the `generate_paper_figures_pytorch.py` file for examples of how to do this.


## Summary of Files
- `pyproject.toml` and associated `uv.lock` contain the package dependencies and versions used in the experiments.  The `requirements.txt` was generated by `uv pip freeze > requirements.txt` for compatibility with `pip` and `conda`.
- `stochastic_growth_baseline_pytorch.py`: Solves the stochastic growth model using a standard algorithm (Euler iteration with linear interpolation) to provide a baseline for comparison.
  - This is by far the slowest part of the code as Python (even with Pytorch or JAX) is not especially convenient for the classic numerical methods used in our baseline.
- `stochastic_growth_pytorch.py`: Solves the stochastic growth model using deep learning as described in the paper.
  - It calls the `stochastic_growth_baseline_pytorch` module to calculate the relative error versus the baseline.
- `generate_paper_figures_pytorch.py`: Generates all figures and numerical results used in the paper.
  - It imports and calls `stochastic_growth_pytorch.py` and contains a summary of all default parameters for easy reference.
  - It saves all output to the `.figures` directory, including a `results.json` file that summarizes numerical results.
- The `stochastic_growth_pytorch_combined.py` file contains a combined version which solves for the baseline and deep learning solutions in one file, and has an accompanying `generate_paper_figures_pytorch_combined.py` file to generate figures.
  - As before, it supports CLI arguments such as `python stochastic_growth_pytorch.py --k_0_multiplier=0.9 --seed=53`
  - The code for the NN solution is identical to the main version.
  - This version is much faster for calculating the baseline and is self-contained.
  - However the baseline solution is using native Pytorch Float32 and a few smaller algorithmic changes which lead to lower accuracy.  With very low euler errors, but roughly of the same magnitude as the deep learning solution.
  - Hence, because relative error calculates may be distorted, so we use the slower but more accurate baseline version for the paper replication.